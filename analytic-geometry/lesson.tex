\documentclass[12pt]{article}

%Packages%
\usepackage{amsmath, bm, xcolor, amssymb, parskip, cancel}

%Meta%
\title{Analytic Geometry}
\author{Diego ROJAS}

%Shortcuts%
\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\R}{{\rm I\!R}}
\newcommand{\Rn}{{\R^n}}
\newcommand{\Rmn}{{\R^{m \times n}}}
\newcommand{\Rnn}{{\R^{n \times n}}}

\newcommand{\bx}{{\bm{x}}}
\newcommand{\by}{{\bm{y}}}
\newcommand{\bz}{{\bm{z}}}
\newcommand{\bb}{{\bm{b}}}
\newcommand{\A}{{\bm{A}}}
\newcommand{\I}{{\bm{I}}}
\newcommand{\B}{{\bm{B}}}
\newcommand{\0}[0]{\bm{0}}

\newcommand{\ipr}[1]{\langle #1 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\vecs}[2]{{\bm{#1}_1, \dots, \bm{#1}_#2}}

\newcommand{\xdefinition}[2]{\paragraph{\colorbox{#1!30}{\textbf{Definition:}} (#2):}}
\newcommand{\xrule}[1]{\paragraph{\colorbox{#1!30}{\textbf{Rule:}}}}

%Document%
\begin{document}

\maketitle

%Introduction%	
\section{Introduction}

%Norms%
\section{Norms}

\xdefinition{blue}{Norm} A norm on a vector space $V$ which assigns each vector $\bx$ its length $\norm{\bx}$ is defined as
%
\begin{equation}
\begin{aligned}
	\norm{\ \cdot \ } : \ &V \rightarrow \R \\
	                      &\bx \mapsto \norm{\bx} 
\end{aligned}
\end{equation}
%
such that for all $\lambda \in \R$ and $\bx,\by \in V$ the following holds
%
\begin{itemize}
	\item \textit{Absolutely homogeneous} $|\lambda| \norm{\bx} = \norm{\lambda \bx}$
	\item \textit{Triange equality} $\norm{\bx + \by} \leqslant \norm{\bx} + \norm{\by}$
	\item \textit{Positive definite} $\norm{\bx} \geqslant 0$ and $\norm{\bx} = 0 \iff \bx = \0$
\end{itemize}

\xdefinition{blue}{Manhattan norm} Also called $\ell_1$ norm, it's defined for $\bx \in \R^n$ as
%
\begin{equation}
	\norm{\bx}_1 = \sum\limits_{i=1}^n |x_i|
\end{equation}

\xdefinition{blue}{Euclidian norm} Also called $\ell_2$ norm, it's defined for $\bx \in \R^n$ as
%
\begin{equation}
	\norm{\bx}_2 = \sqrt{\sum\limits_{i=1}^{n} x_i^2} = \sqrt{\bx^\top \bx}
\end{equation}

%Inner Products%
\section{Inner Products}

\xdefinition{blue}{Dot product} A \textit{dot product} or \textit{scalar product} is a type of inner product. In $\R^n$ it is defined as
%
\begin{equation}
	\bx^\top \by = \sum\limits_{i=1}^{n} x_i y_i
\end{equation}

\xdefinition{green}{Bilinear mapping} A \textit{bilinear mapping} is a mapping $\Omega$ with two arguments for which the following holds
%
\begin{equation}
\begin{aligned}
	\Omega(\lambda \bx + \psi \by, \bz) &= \lambda \Omega(\bx, \bz) + \psi\Omega(\by, \bz) \\
	\Omega(\bx, \lambda \by + \psi\bz) &= \lambda \Omega(\bx, \by) + \psi\Omega(\bx, \bz)
\end{aligned}
\end{equation}

\xrule{blue} Given $\Omega : V \times V \rightarrow \R$ then
%
\begin{itemize}
	\item $\Omega$ is called \textit{symmetric} if $\Omega(\bx, \by) = \Omega(\by, \bx)$
	\item $\Omega$ is called \textit{positive definite} if
	\begin{equation}
		\forall \bx \in V \backslash \{\0\} : \Omega(\bx, \bx) > 0, \ \ \Omega(\0, \0) = 0
	\end{equation}
	\item A positive definite, symmetric bilinear mapping $\Omega$ is called an \textit{inner product} and is written as $\ipr{\bx, \by}$.
	\item The pair $(V, \ipr{\cdot, \cdot})$ is called \textit{inner product space} or \textit{Euclidian vector space} if $\ipr{\cdot, \cdot}$ is defined as the $\ell_2$ norm.
\end{itemize}

\xdefinition{blue}{Symmetric, positive definite matrix} A symmetric matrix $\A \in \R^{n \times n}$ that satisfies
%
\begin{equation}
	\forall \bx \in V \backslash \{\0\} : \bx^\top \A \bx > 0
\end{equation}
%
If only $\geqslant$ holds, it is \textit{symmetric, positive semidefinite}.

\xrule{blue} Let $V$ be a vector space with basis $B$. It holds that $\ipr{\cdot, \cdot} : V \times V \rightarrow \R$ is an inner product if and only if there exists a symmatric positive definite matrix $\A \in \R^{n \times n}$ with
%
\begin{equation}
	\ipr{\bx, \by} = \hat{\bx}^\top \A \hat{\by}
\end{equation}
%
where $\hat{\bx}, \hat{\by}$ are the coordinate vectors of $\bx, \by$ with respect to $B$. Consequently, the following holds for $\A$:
%
\begin{itemize}
	\item The null space of $\A$ consist only of $\0$.
	\item The diagonal elements $a_{ij}$ of $\A$ are positive.
\end{itemize}

%Lengths and Distances%
\section{Lengths and Distances}

\xrule{red} Every inner product $\ipr{\bx, \bx}$ induces a norm $\norm{\bx} := \sqrt{\ipr{\bx, \bx}}$.

\xdefinition{red}{Cauchy-Schwartz inequality} For an inner product vector space $(V, \ipr{\cdot, \cdot})$, the induced norm $\norm{\ \cdotÂ \ }$ satisfies the \textit{Cauchy-Schwartz inequality} if
%
\begin{equation}
	| \ipr{\bx, \by} | \leqslant \norm{\bx} \norm{\by}
\end{equation}

\xdefinition{blue}{Distance and Metric} The \textit{distance} between two vectors $\bx, \by$ is defined as
%
\begin{equation}
	d(\bx, \by) := \norm{\bx - \by} = \sqrt{\ipr{\bx - \by, \bx - \by}}
\end{equation}
%
If we use the dot product as the inner product then $d$ is called the \textit{Euclidian distance}. The mapping
%
\begin{equation}
\begin{aligned}
	d : \ &V \times V \rightarrow \R
	      &(\bx, \by) \mapsto d(\bx, \by)
\end{aligned}
\end{equation}
%
is called a \textit{metric} and must satisfies the following properties.
%
\begin{itemize}
	\item \textit{Positive definite} $d(\bx, \by) \geqslant 0, \ d(\bx, \by) = 0 \iff \bx = \by$
	\item \textit{Symmetric} $d(\bx, \by)= d(\by, \bx)$
	\item \textit{Triangle equality} $d(\bx, \bz) \leqslant d(\bx, \by) + d(\by, \bz)$
\end{itemize}

%Angles and Orthogonality%
\section{Angles and Orthogonality}

\xrule{red} Given $\bx, \by \neq \0$. The angle $\omega$ between $\bx$ and $\by$ is defined as
%
\begin{equation}
	\cos \omega = \frac{\ipr{\bx, \by}}{\norm{\bx} \ \norm{\by}}
\end{equation}
%
and is contained inside $[-1, 1]$.

\xdefinition{red}{Orthogonality} Two vectors $\bx, \by$ are \textit{orthogonal} if $\ipr{\bx, \by} = 0$. We write $\bx \perp \by$. Furthermore, $\bx, \by$ are \textit{orthonormal} if $\norm{\bx} = 1 = \norm{\by}$.

\xrule{red}
%
\begin{itemize}
	\item Whether two vectors are orthogonal depends on the norm being used.
	\item The $\0$ vector is orthogonal to every vector in the vector space.
	\item Orthogonality is a generalization of the concept of perpendicularity.
\end{itemize}

\xdefinition{red}{Orthogonal matrix} A matrix $\A \in \Rnn$ is orthogonal if
%
\begin{equation}
	\A \A^{\top} = \I = \A^{\top} \A
\end{equation}
%
which implies that
%
\begin{equation}
	\A^{-1} = \A^{\top}
\end{equation}

%Orthonormal basis%
\section{Orthonormal Basis}

\xdefinition{red}{Orthonormal basis} A basis $B$ of a vector space $V$ is said to be \textit{orthonormal} if
%
\begin{equation}
\begin{aligned}
	\ipr{\bb_i, \bb_j} &= 0 \ \text{for} \ i \neq j \\
	\ipr{\bb_i, \bb_i} &= 1
\end{aligned}
\end{equation}
%
In other words, all column vectors of $B$ are orthogonal to each other. This implies that every vector has length/norm $1$, if this is not the case the basis is only said to be \textit{orthogonal}.

\xdefinition{red}{Gram-Schmidt process} Given a basis $\hat{\B}$, we can perform Gaussian eliminiation on the augmented matrix $[\hat{\B} \hat{\B}^\top | \hat{\B}]$ to obtain an orthonormal basis.

\section{Orthogonal Complement}

\xdefinition{red}{Orthogonal complement} Given an $D$ dimensional vector space $V$ and an $M$ dimensional subspace $U \in V$. The \textit{orthogonal complement} $U^{\perp}$ is a subspace of $V$ with dimension $D - M$ that contains all vectors in $V$ that are orthogonal to every vector in $U$ with $U \cap U^\perp$ so that any vector $\bx$ can be uniquely decomposed into
%
\begin{equation}
	\bx = \sum\limits_{i=1}^{M} \lambda_i \bb_i \ + \sum\limits_{j=1}^{D-M} \psi_j \bb^{\perp}_j
\end{equation}
%
where $(\vecs{\bb}{M})$ is the basis of $U$, $(\vecs{\bb^{\perp}}{{D-M}})$ is the basis of $U^{\perp}$.

\xrule{red} Let $U$ be a two dimensional subspace of a three dimensional vector space $V$. $U$ is a plane of $V$. Then the vector $\bm{w}$ where $\norm{\bm{w}} = 1$, which is orthogonal to the plane $U$ is the basis vector of $U^{\perp}$.

%Inner Products of Functions%
\section{Inner Products of Functions}

\xrule{red} An inner product of two functions $u : \R \rightarrow \R, v : \R \rightarrow \R$ can be defined as the definite integral
%
\begin{equation}
	\ipr{u, v} := \int_b^a u(x) v(x) dx
\end{equation}

\section{Orthogonal projections}

\xdefinition{red}{Projection} Given a vector space $V$ and a subspace $U \subseteq V$, a \textit{projection} is a linear mapping $\pi : V \rightarrow U$ such that $\pi^2 = \pi \circ \pi = \pi$.
%
The transformation matrix of a projection is called a \textit{projection matrix} denoted $\bm{P}_\pi$ such that $\bm{P}_\pi^2 = \bm{P}_\pi$.j

\xrule{red} Given a vector space $V \in \Rn$, a subspace $U \in \Rn, U \subseteq V$ with basis $\bb$. When we project $\bx \in V$ onto $U$ we seek the vector $\pi_U(\bx) \in U$.
%
\begin{itemize}
	\item The projection $\pi_U(\bx)$ is one for which $\norm{\bx - \pi_U(\bx)}$
	\item The segment $\bx - \pi_U(\bx)$ is orthogonal to $U$ and its basis.
	\item $\ipr{\pi_U(\bx) - \bx, b} = 0$.
	\item $\pi_U(\bx) = \lambda \bb$.
\end{itemize}
%
Furthermore, the following equations hold.
%
\begin{equation}
	\lambda = \frac{\ipr{\bx, \bb}}{\bb, \bb}
\end{equation}
%
\begin{equation}
	
\end{equation}

\end{document}
