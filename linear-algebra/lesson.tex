\documentclass[12pt]{article}

%Packages%
\usepackage{amsmath, bm, xcolor, amssymb, parskip, cancel}

%Meta%
\title{Linear Algebra}
\author{Diego ROJAS}

%Shortcuts%
\newcommand{\R}{{\rm I\!R}}
\newcommand{\N}{{\rm I\!N}}
\newcommand{\bx}{{\bm{x}}}
\newcommand{\A}{{\bm{A}}}
\newcommand{\B}{{\bm{B}}}
\newcommand{\vecs}[2]{{\bm{#1}_1, \dots, \bm{#1}_#2}}
\newcommand{\xdefinition}[2]{\paragraph{\colorbox{#1!30}{\textbf{Definition:}} (#2):}}
\newcommand{\xrule}[1]{\paragraph{\colorbox{#1!30}{\textbf{Rule:}}}}
\newcommand{\mc}[1]{\mathcal{#1}}

%Document%
\begin{document}

\maketitle

%Introduction%	
\section{Introduction}

Linear algebra is a discipline of mathematics that deals with linear equations, vectors, matrices and their transformations.

%Systems of linear equations%
\section{Systems of linear equations}

\xdefinition{green}{Linear system} A collection of one or more equations involving the same variables.

\xrule{green} For any linear system of the following form:
%
\begin{equation}
\begin{aligned}
	a_{11} x_1 + \cdots +& a_{1n} x_n = b_1 \\
    	         \vdots & \\
	a_{m1} x_1 + \cdots +& a_{mn} x_n = b_m
\end{aligned}
\end{equation}
%
There exists a shorthand notation:
%
\begin{equation}
\begin{aligned}
\begin{bmatrix}
    a_{11} \\
    \vdots \\
    a_{m1}
\end{bmatrix} x_1 + \begin{bmatrix}
    a_{12} \\
    \vdots \\
    a_{m2}
\end{bmatrix} x_1 + \cdots + \begin{bmatrix}
    a_{1n} \\
    \vdots \\
    a_{mn}
\end{bmatrix} x_n = \begin{bmatrix}
    b_{1} \\
    \vdots \\
    b_{m}
\end{bmatrix}
\end{aligned}
\end{equation}
%
Which can be further compacted into the form $\A \bm{x} = \bm{b}$ like so:
%
\begin{equation}
\begin{aligned}
\begin{bmatrix}
    a_{11} & \cdots & a_{1n} \\
    \vdots & & \vdots      \\
    a_{m1} & \cdots & a_{mn}
\end{bmatrix} \begin{bmatrix}
    x_{1} \\
    \vdots \\
    x_{n}
\end{bmatrix} = \begin{bmatrix}
    b_{1} \\
    \vdots \\
    b_{m}
\end{bmatrix}
\end{aligned}
\end{equation}
%
Or using the \textit{augmented matrix} notation $[\A | \bm{b}]$
%
\begin{equation}
\left[\begin{array}{ccc|c}
    a_{11} & \cdots & a_{1n} & b_{1}  \\
    \vdots &        & \vdots & \vdots \\ 
    a_{m1} & \cdots & a_{mn} & b_{m}
\end{array}\right]
\end{equation}

%Matrices%
\section{Matrices}

\xdefinition{green}{Matrix} With $m, n \in \N$ a real-valued $(m, n)$ matrix $\A$ is an $m \cdot n$ tuple of elements $a_{ij}$. $\R^{m \times n}$ is the set of all real valued matrices.

\xrule{green} For matrices $\bm{A} \in \R^{m \times n}, \bm{B} \in \R^{n \times k}$, the elements $c_{ij}$ of the product $\bm{C} = \bm{AB} \in \R^{m \times k}$ are computed as:
%
\begin{equation}
	c_{ij} = \sum\limits_{l=1}^{n} = a_{il} b_{lj}, \text{ where } i = 1, \dots, m, \text{ and } j = 1, \dots, k.
\end{equation}

\xrule{green} For matrices $\A, \bm{B} \in \R^{m \times n}$, addition is defined as element-wise.

\xrule{green} Element wise multiplication of $\bm{A}, \bm{B} \in \R^{m \times n}$ is called a \textit{Hadamard product}.

\xdefinition{green}{Identity matrix} We define an identity matrix in $\R^{n \times n}$ as:
%
\begin{equation}
I_n := \begin{bmatrix}
    1 & \cdots & 0 & \cdots & 0 \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & \cdots & 1 & \cdots & 0 \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & \cdots & 0 & \cdots & 1
\end{bmatrix} \in \R^{n \times n}
\end{equation}

\xdefinition{green}{Inverse matrix} The inverse of a matrix $\bm{A} \in \R^{n \times n}$ denoted $\bm{A}^{-1}$ is a unique matrix $\bm{B} \in \R^{n \times n}$ given $\bm{AB} = \bm{I}_n = \bm{BA}$. If there exists an inverse for a matrix $\A$ then it is called \textit{regular/invertible/nonsingular}, otherwise \textit{singular/non-invertible}.

\xdefinition{green}{Transpose} For $\bm{A} \in \R^{m \times n}$ the matrix $\bm{B} \in \R^{n \times m}$ with $b_{ij} = a_{ji}$ is called the transpose of $\bm{A}$. We write $\bm{B} = \bm{A}^\top$.

%Solving systems of linear equations%
\section{Solving systems of linear equations}

\xrule{green} When solving a system of linear equations of the form $\bm{Ax} = \bm{b}$ we're looking to find scalars $x_1, \dots, x_n$ such that $\sum\nolimits_{i=1}^{n} x_i \bm{c}_i = \bm{b}$. A \textit{particular solution} of a linear system is a solution to the equation with particular, arbitrary values. A \textit{general solution} of a system of linear equations is a formula which gives all solutions for different values of parameters.

\xdefinition{green}{Elementary transformations} Consists of three rule that can be applied on a linear system of the form $\A \bm{x} = \bm{b}$.
%
\begin{itemize}
	\item Exchange of two equations (swapping rows in $[\A | \bm{b}]$)
	\item Multiplication of an equation with a constant $\lambda \in \R \backslash \{0\}$
	\item Addition of two equations (rows of $[\A | \bm{b}]$)
\end{itemize}
%
We use $\leadsto$ to indicate an elementary transformation.

\xdefinition{blue}{Pivot $|$ Leading coefficient} The \textit{leading coefficient} of a row (first nonzero number from the left) is called the \textit{pivot} and is always strictly to the right of the pivot of the row above it.

\xdefinition{blue}{Row echelon form} A matrix being in row echelon form means that Gaussian elimination has operated on the rows or columns of that matrix such that:
%
\begin{itemize}
	\item All rows consisting of only zeroes are at the bottom.
	\item The leading coefficient of a non-zero row is always strictly to the right of the leading coefficient of the row above it.
\end{itemize}

\xdefinition{blue}{Reduced row echelon form} An equation is in \textit{reduced row echelon form} (or \textit{row cannonical form}) if every pivot is $1$, the pivot is the only nonzero entry in its column (each pivot's column is a unit vector).

\xrule{blue} The variables corresponding to the pivots in the row-echelon form are called \textit{basic variables} and the other variables are \textit{free variables}.

\xrule{red} To obtain the inverse $\A^{-1}$ of a matrix $\A$ we can use Gaussian elimination to bring it into reduced row echelon form such as the desired inverse is given as its right hand side.
% 
\begin{equation}
	[\bm{A}|\bm{I}_n] \leadsto \cdots \leadsto [\bm{I}_n|\bm{A}^{-1}]
\end{equation}

%Vector spaces%
\section{Vector spaces}

\xdefinition{green}{Group} Given a set $\mc{G}$ and an operation $\otimes$ defined on $\mc{G}$. $G := (\mc{G}, \otimes)$ is a group if the following holds:
%
\begin{itemize}
	\item Closure of $\mc{G}$ under $\otimes$: $\forall x, y \in \mc{G} : x \otimes y \in \mc{G}$
	\item Associativity: $\forall x, y, z \in \mc{G} : (x \otimes y) \otimes z = x \otimes (	y \otimes z)$
	\item Neutral element: $\exists e \in \mc{G} \ \forall x \in \mc{G} : x \otimes e = x$
	\item Inverse element: $\forall x \in \mc{G} \ \exists y \in \mc{G}: x \otimes y = e$ where $e$ is the neutral element.
\end{itemize}

\xdefinition{blue}{Vector space} A real-valued vector space $V = (\mc{V}, +, \cdot)$ is a set $\mc{V}$ with two operations $+$ and $\cdot$ defined where the following holds:
%
\begin{itemize}
	\item $(\mc{V}, +)$ is an abelian group.
	\item Distributivity:
	\begin{itemize}
	\item $\forall \bm{x}, \bm{y} \in \mc{V} \ \forall \lambda \in \R : \lambda(\bm{x} + \bm{y}) = \lambda \bm{x} + \lambda \bm{y}$
	\item $\forall \bm{x} \in \mc{V} \ \forall \lambda, \psi \in \R : (\lambda + \psi) \bm{x} = \lambda \bm{x} + \psi \bm{y}$
	\end{itemize}
	\item Associativity (outer operation)
	\item Neutral element (outer operation)
\end{itemize}
%

\xdefinition{blue}{Vector subspace} Let $V = (\mc{V}, +, \cdot)$ be a vector space and $\mc{U} \subseteq \mc{V}, \mc{U} \neq \cancel{0}$. Then $U = (\mc{U}, +, \cdot)$ is a \textit{vector subspace} (or \textit{linear subspace}) if $U$ is a vector space restricted to $\mc{U} \times \mc{U}$ and $\mc{U} \times \R$. We write $U \subseteq V$.

\xrule{blue} Given $U \subseteq V$, the following properties of $V$ are passed to $U$.
%
\begin{itemize}
	\item Abelian group, distributivity, associativity and neutral element properties.
	\item $\mc{U} \neq \cancel{0}$, in particular: $\bm{0} \in \mc{U}$.
	\item Closure of $U$:
	\begin{itemize}
		\item $\forall \lambda \in \R \ \forall \bm{x} \in \mc{U} : \lambda \bm{x} \in \mc{U}$.
		\item $\forall \bm{x}, \bm{y} \in \mc{U} : \bm{x} + \bm{y} \in \mc{U}$.
	\end{itemize}
\end{itemize}

\xrule{blue} Every subspace $U \subseteq (\R^n, +, \cdot)$ is the solution space of a homogeneous system of linear equations $\A \bm{x} = \bm{0}$ for $\bm{x} \in \R^n$

%Linear independence%
\section{Linear independence}

\xdefinition{blue}{Linear combination} Consider a vector space $V$ and a finite number of vectors $\bx_1, \dots, \bx_k \in V$. Then every $\bm{v} \in V$ of the form:
%
	\begin{equation}
		\bm{v} = \sum\limits_{i=1}^{k} \lambda_i \bm{x}_i \in V
	\end{equation}
%
	with $\lambda_1, รท\dots, \lambda_k \in \R$ is a linear combination of the vectors $\bx_1, \dots, \bx_k$.

\xdefinition{red}{Linear independence} Given a vector space $V$ and $k \in \N$. Vectors $\bx_1, \dots, \bx_k \in V$ are said to be linearly independent if there exists no non-trivial solution to $\bm{0} = \sum\nolimits_{i=1}^{k} \lambda_i \bx_i$ with at least one $\lambda_i \neq 0$. Otherwise they're linearly dependent.

\xrule{red}
%
\begin{itemize}
  \item If at least one of the vector $\bm{x}_i$ is $\bm{0}$ then they are linearly dependent.
  \item The vectors $\{\bx_1, \dots, \bx_k : \bx_i \neq \bm0, i =1, \dots, k\} , k \geqslant 2$ are linearly dependent, if and only if, at least one of them is a linear combination of the others.
  \item To check whether $\bx_1, \dots, \bx_k \in V$ are linearly independent we can use Gaussian elimation: write all vectors as columns of a matrix $\A$ and perform Gaussian elimination until the matrix is in row-echelon form.\begin{itemize}
  \item Non-pivot columns can be expressed as linear combinations of vectors on their left.
  \item Pivot columns are linearly independent from vectors on their left.
	  \end{itemize}
	  If all columns are pivots, the vectors are linearly independent.
\end{itemize}

\xrule{red} Given $m$ linear combinations over $k$ linearly independent vectors $\bm{b}_1, \dots, \bm{b}_k \in V$.

\begin{equation}
	\begin{aligned}
		\bx_1 &= \sum\limits_{i=1}^k \lambda_{i1} \bm{b}_i \\
		\vdots & \\
		\bx_m &= \sum\limits_{i=1}^k \lambda_{im} \bm{b}_i \\
	\end{aligned}
\end{equation}
%
We can write, with $\B = [\bm{b}_1, \dots, \bm{b}_k]$, the following:
%
\begin{equation}
	\bx_j = \bm{B} \bm{\lambda}_j, \ \ \bm{\lambda}_j = \begin{bmatrix}
		\lambda_1j \\
		\vdots \\
		\lambda_kj
	\end{bmatrix}, \ \ j = 1, \dots,m 
\end{equation}
%
We can test whether $\bx_1, \dots, \bx_m$ are linearly independent using:
\begin{equation}
	\sum\limits_{j=1}^m \psi_j \bx_j = \sum\limits_{j=1}^m \B\bm{\lambda}_j = \B \sum\limits_{j=1}^m \psi_j \bm{\lambda}_j
\end{equation}
%
Which means that $\{\vecs{x}{k}\}$ is linearly independent if the column vectors $\{\vecs{\lambda}{m}\}$ are linearly independent.

\xrule{red} In a vector space $V$, $m$ linear combinations of $\vecs{b}{k}$ are linearly independent if $m > k$. 

%Basis and rank%
\section{Basis and rank}

In a vector space $V$, we are interested in a set of vectors $\mathcal{A}$ that posess the property that any vector $\bm{v} \in V$ can be obtained through a linear combination of vectors in $\mathcal{A}$.

%Generating Set and Basis%
\subsection{Generating Set and Basis}

\xdefinition{blue}{Generating Set and Span} Given a set of vectors $\mc{A} = \{ \vecs{x}{k} \} \subseteq \mc{V}$. If every vector $\bm{v} \in \mc{V}$ can be expressed as a linear combination of vectors in $\mc{A}$, $\mc{A}$ is called a \textit{generating set} of $V$.	The set of all linear combinations of vectors in $\mc{A}$ is called the span of $\mc{A}$. We write $V = \text{span}[A]$ or $V = \text{span}[\vecs{x}{k}]$.

\xdefinition{red}{Basis} Consider a vector space $V = (\mc{V}, +, \cdot)$ and $\mc{A} \subseteq \mc{V}$. A generating set $\mc{A}$ of $\mc{V}$ is called \textit{minimal} if there exists no smaller set $\tilde{\mc{A}} \subsetneq \mc{A} \subseteq \mc{V}$ that spans $\mc{V}$. Every linearly independent generating set of $V$ is minimal and is called a \textit{basis} of $V$.

\xrule{red} Given $\mc{B} \subseteq \mc{V}, \mc{B} \neq \emptyset$. Then the following statements are equivalent:
%
\begin{itemize}
	\item $\mc{B}$ is a basis of $V$
	\item $\mc{B}$ is a minimal generating set.
	\item $\mc{B}$ is a maximal linearly independent set of vectors in $V$ such that adding any vector to the set $\mc{B}$ would make it linearly dependent. 
	\item Every vector $\bm{x} \in V$ is a linear combination of vectors from $\mc{B}$, and every combination is unique, i.e., with
		\begin{equation}
			\bm{x} = \sum\limits_{i=1}^{k} \lambda_i \bm{b}_i
		\end{equation}
\end{itemize}

\xrule{red} Every vector space $V$ posseses a basis $B$. There can be many bases for $V$, all have the same number of elements, the \textit{basis vectors}.

\xrule{red} A basis of a subspace $U = span[\vecs{x}{k}] \subseteq \R^n$ by:
%
\begin{itemize}
	\item Write the spanning vectors as columns of a matrix $\A$
	\item Determine the row-echelon form of $\A$
	\item The spanning vectors associated with the pivots columns are a basis of $U$. 
\end{itemize}

\subsection{Rank}

\xdefinition{red}{Rank} The number of linearly independent columns of a matrix $\A \in \R^{m \times n}$ equals the number of linearly indepdendent rows and is called the \textit{rank}. We write $\text{rk}(A)$

\xrule{red}
%
\begin{itemize}
	\item $\text{rk}(\A)$ = $\text{rk}(\A^\top)$ .i.e column rank equals row rank.
	\item The columns of $\A \in \R^{m \times n}$ span a subspace $U \subseteq \R^m$ with $\text{dim}(U) = \text{rk}(\A)$. This subspace is called the \textit{image} or \textit{range}. A basis of $U$ can be found by applying Gaussian elimination to $\A$. Same goes for the rows, given a subspace $W \in \R^n$.	
	\item For all $\A \in \R^{n \times n}$ it holds that $\A$ is regular (invertible) if and only $\text{rk}(\A) = n$.
	\item For all $\A \in \R^{m \times n}$ and all $\bm{b} \in \R^m$ it holds that the linear equation system $\A \bm{x} = \bm{x}$ can be solved only and if $\text{rk}(\A) = \text{rk}(\A | \bm{b})$
	\item A matrix $\A \in \R^{m \times n}$ is said to have \textit{full-rank} if $\text{rk}(\A) = \text{min}(m, n)$. Otherwise, it is \textit{rank-deficient}.
\end{itemize}

%Linear mappins%
\section{Linear mappings}

\xdefinition{blue}{Linear mapping} For vector spaces $V, W$, a mapping $\Phi : V \rightarrow W$ is called a \textit{linear mapping} (or \textit{linear transformation} or \textit{vector space homomorphism}). Linear transformations are generally represented as matrices.
%
\begin{equation}
\begin{aligned}
	\Phi(\bm{x} + \bm{y}) &= \Phi(\bm{x}) + \Phi(\bm{y}) \\
	\Phi(\lambda \bm{x}) &= \lambda \Phi(\bm{x})
\end{aligned}
\end{equation}

\xdefinition{blue}{Injective, Surjective, Bijective} Consider a mapping $\Phi : \mc{V} \rightarrow \mc{W}$ where $\mc{V}, \mc{W}$ can be arbitrary sets. Then $\Phi$ is called
%
\begin{itemize}
	\item \textit{Injective} if $\forall \bm{x} \bm{y} \in \mc{V} : \Phi(\bm{x}) = \Phi(\bm{y}) \implies \bm{x} = \bm{y}$.
	\item \textit{Surjective} if $\Phi(\mc{V}) = \mc{W}$.
	\item \textit{Bijective} if it is injective and surjective. Which means there exists a mapping $\Psi$ so that $\Psi \circ \Phi(\bm{x}) = \bm{x}$. This mapping $\Psi$ is called the inverse of $\Phi$ and denoted $\Phi^{-1}$.
\end{itemize}

\xrule{blue}
%
\begin{itemize}
	\item \textit{Isomorphism} $\Phi : V \rightarrow W$ linear and bijective.
	\item \textit{Endomorphism} $\Phi : V \rightarrow V$ linear.
	\item \textit{Automorphism} $\Phi : V \rightarrow V$ linear and bijective.
	\item We define $\text{id}_V : V \rightarrow V, \bm{x} \mapsto \bm{x}$ as the \textit{identity mapping} or \textit{identity automorphism} in V.
\end{itemize}

\xrule{blue} \textit{Finite dimensional vector spaces} $\mc{V}$ and $\mc{W}$ are \textit{isomorphic} ($\mc{V} \rightarrow \mc{W}$ linear and bijective) if and only if $\dim(\mc{V})$ = $\dim(\mc{W})$.

\xdefinition{blue}{Ordered basis} Matrix representation of the basis vectors for an $n$ dimensional vector space $\mc{V}$. We write:
%
\begin{equation}
\begin{aligned}
	\text{Ordered basis notation: } B &= (\vecs{b}{n}) \\
	\text{Matrix notation: } \bm{B} &= [\vecs{b}{n}] \\
	\text{Unordered basis notation: } \mc{B} &= \{\vecs{b}{n}\}
\end{aligned}
\end{equation}

\xdefinition{blue}{Coordinates} Given an ordered basis $B = (\vecs{b}{n})$ The \textit{coordinate vector}/\textit{coordinate representation} of $\bm{x} \in V$ is given as:
%
\begin{equation}
	\bm{\alpha} = \begin{bmatrix}
		\alpha_1 \\
		\vdots \\
		\alpha_n
	\end{bmatrix} \in \R^n
\end{equation}
%
Where
%
\begin{equation}
	\bm{x} = \alpha_1 \bm{b}_1 + \dots + \alpha_n \bm{b}_n
\end{equation}

\xdefinition{blue}{Transformation Matrix} Considering vector spaces $V, W$ with corresponding (ordered) bases $B = (\vecs{b}{n})$ and $C = (\vecs{c}{m})$ and a linear mapping $\Phi : V \rightarrow W$. For $j \in \{1, \dots, n\}$:
%
\begin{equation}
	\Phi(\bm{b}_j) = \alpha_{1j} \bm{c}_1 + \cdots + \alpha_{mj} \bm{c}_m
\end{equation}
%
is the unique representation of $\Phi(\bm{b}_j)$ with respect to $C$. Then we call the $m \times n$ matrix $\A_\Phi$, whose elements are given by
%
\begin{equation}
	A_\Phi(i, j) = \alpha_{ij}
\end{equation}
%
the transformation matrix of $\Phi$ (with respect to $B, V, C, W$). Therefore, if $\hat{x}$ is the coordinate vector of $x \in \mc{V}$ with respect to $B$ and $\hat{y}$ the coordinate vector of $y = \Phi(\bm{x}) \in \mc{W}$ with respect to $C$, then
%
\begin{equation}
	\hat{\bm{y}} = \A_\Phi \hat{\bm{x}}
\end{equation}

\xdefinition{red}{Basis change} For a linear mapping $\Phi : V \rightarrow W$, the ordered bases $B, \tilde{B}$ of $V$ of $n$ elements and $C, \tilde{C}$ of $W$ of $m$ elements and the transformation matrix $\A_\Phi$ of $\Phi$ with respect to $B$ and $C$, $\tilde{\A}_\Phi$ with respect to $\tilde{B}$ and $\tilde{C}$ is given as
%
\begin{equation}
	\tilde{\A}_\Phi = \bm{T}^{-1} \A_\Phi \bm{S}
\end{equation}
%
where $\bm{S} \in \R^{n \times n}$ is the transformation matrix of $\text{id}_V$ and $\bm{T} \in \R^{m \times m}$ is the transformation matrix of $\text{id}_W$

\end{document}











































