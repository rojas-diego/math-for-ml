\documentclass[12pt]{article}

%Packages%
\usepackage{amsmath, bm, xcolor, amssymb, parskip}

%Meta%
\title{Linear Algebra}
\author{Diego ROJAS}

%Shortcuts%
\newcommand{\R}{{\rm I\!R}}
\newcommand{\N}{{\rm I\!N}}
\newcommand{\bx}{{\bm{x}}}
\newcommand{\A}{{\bm{A}}}
\newcommand{\B}{{\bm{B}}}
\newcommand{\vecs}[2]{{\bm{#1}_1, \dots, \bm{#1}_#2}}
\newcommand{\xdefinition}[2]{\paragraph{\colorbox{#1!30}{\textbf{Definition:}} (#2):}}
\newcommand{\xrule}[1]{\paragraph{\colorbox{#1!30}{\textbf{Rule:}}}}

%Document%
\begin{document}

\maketitle

%Introduction%	
\section{Introduction}

Linear algebra is a discipline of mathematics that deals with linear equations, vectors, matrices and their transformations.

%Systems of linear equations%
\section{Systems of linear equations}

%Matrices%
\section{Matrices}

%Solving systems of linear equations%
\section{Solving systems of linear equations}

%Vector spaces%
\section{Vector spaces}

%Linear independence%
\section{Linear independence}

\xdefinition{red}{Linear combination} Consider a vector space $V$ and a finite number of vectors $\bx_1, \dots, \bx_k \in V$. Then every $\bm{v} \in V$ of the form:
%
	\begin{equation}
		\bm{v} = \sum\limits_{i=1}^{k} \lambda_i \bm{x}_i \in V
	\end{equation}
%
	with $\lambda_1, รท\dots, \lambda_k \in \R$ is a linear combination of the vectors $\bx_1, \dots, \bx_k$.

\xdefinition{red}{Linear independence} Given a vector space $V$ and $k \in \N$. Vectors $\bx_1, \dots, \bx_k \in V$ are said to be linearly independent if there exists no non-trivial solution to $\bm{0} = \sum\nolimits_{i=1}^{k} \lambda_i \bx_i$ with at least one $\lambda_i \neq 0$. Otherwise they're linearly dependent.

\xrule{red}
%
\begin{itemize}
  \item If at least one of the vector $\bm{x}_i$ is $\bm{0}$ then they are linearly dependent.
  \item The vectors $\{\bx_1, \dots, \bx_k : \bx_i \neq \bm0, i =1, \dots, k\} \geqslant 2$ are linearly dependent, if and only if, at least one of them is a linear combination of the others.
  \item To check whether $\bx_1, \dots, \bx_k \in V$ are linearly independent we can use Gaussian elimation: write all vectors as columns of a matrix $\A$ and perform Gaussian elimination until the matrix is in row-echelon form.\begin{itemize}
  \item Non-pivot columns can be expressed as linear combinations of vectors on their left.
  \item Pivot columns are linearly independent from vectors on their left.
	  \end{itemize}
	  If all columns are pivots, the vectors are linearly independent.
\end{itemize}

\xrule{red} Given $m$ linear combinations over $k$ linearly independent vectors $\bm{b}_1, \dots, \bm{b}_k \in V$.

\begin{equation}
	\begin{aligned}
		\bx_1 &= \sum\limits_{i=1}^k \lambda_{i1} \bm{b}_i \\
		\vdots & \\
		\bx_m &= \sum\limits_{i=1}^k \lambda_{im} \bm{b}_i \\
	\end{aligned}
\end{equation}
%
We can write, with $\B = [\bm{b}_1, \dots, \bm{b}_k]$, the following:
%
\begin{equation}
	\bx_j = \bm{B} \bm{\lambda}_j, \ \ \bm{\lambda}_j = \begin{bmatrix}
		\lambda_1j \\
		\vdots \\
		\lambda_kj
	\end{bmatrix}, \ \ j = 1, \dots,m 
\end{equation}
%
We can test whether $\bx_1, \dots, \bx_m$ are linearly independent using:
\begin{equation}
	\sum\limits_{j=1}^m \psi_j \bx_j = \sum\limits_{j=1}^m \B\bm{\lambda}_j = \B \sum\limits_{j=1}^m \psi_j \bm{\lambda}_j
\end{equation}
%
Which means that $\{\vecs{x}{k}\}$ is linearly independent if the column vectors $\{\vecs{\lambda}{m}\}$ are linearly independent.

\xrule{red} In a vector space $V$, $m$ linear combinations of $\vecs{b}{k}$ are linearly independent if $m > k$. 

%Basis and rank%
\section{Basis and rank}

In a vector space $V$, we are interested in a set of vectors $\mathcal{A}$ that posess the property that any vector $\bm{v} \in V$ can be obtained through a linear combination of vectors in $\mathcal{A}$.

%Generating Set and Basis%
\subsection{Generating Set and Basis}

\end{document}

























































